{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b037e9c7-dec9-4a21-b80f-d33ad0fbfca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Bienvenidos al Demo del Pipeline de Cálculo Automático de Balances de Energía EONTEC\n",
    "\n",
    "##Introduccion\n",
    "\n",
    "Los balances de Energía se calculan con un programa de despachos y la información real de despacho de los generadores, la primera se obtiene de XM el ente regulador del mercado de energía de Colombia y el segundo, de los reportes periódicos que saca el generador, generalmente son diarios.\n",
    "\n",
    "Tambien se averigua la diferencia del plan vs lo real, y se puede obtener el valor de la energía generada real, vs la remanente que hace falta, y que hay que cubrir con contratos de energía. \n",
    "\n",
    "*\"Si me sobra vendo la Energía, si me falta, la tengo que comprar\"*\n",
    "\n",
    "En este proceso la **ETL** se encargará de extraer la información de las fuentes mencionadas arriba, de forma remota, posteriormente, la transformará a un formato manejable, la cruzará y filtrará, y finalmente la depositará o la disponibilizará para descarga en un destino para que los analistas o el regulador la usen.\n",
    "\n",
    "###Fuentes\n",
    "\n",
    "- Descarga Archivo de despacho horario de las centrales de energia consolidado\n",
    "- Consumo de API de despacho programado para la fecha del ente regulador.\n",
    "- Consumo de API de Precio de la energia por kWh para el día asignado\n",
    "\n",
    "###Transformaciones\n",
    "\n",
    "- Cambio de formato de Excel a Spark Columnar\n",
    "- Cambio de formato de JSON a Spark Columnar de datos de las API.\n",
    "- Combinación de datos utilizando SQL.\n",
    "- Agregación, cálculos de los balances por unidad de generación (por planta)\n",
    "- Identificación de operación: Compra o Venta.\n",
    "\n",
    "###Transferencias o Cargas\n",
    "- Carga por web a Bucket o Carpeta en la nube del regulador\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a34c6521-ceb6-421e-bc3a-7cc876d20f23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "\n",
    "Lee el código fuente e interprétalo, realiza preguntas a los ingenieros, a los profesores o al arquitecto, en caso que no lo entiendas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d734c9d-118e-483e-a1eb-d8471479df7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Preparación de prerequisitos\n",
    "\n",
    "En esta sección se instalan los paquetes, librerías y componentes que le permitirán al pipeline realizar sus procesos internos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f38d03-89b1-44c5-a3df-fc16b2f4787f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "pip install openpyxl xlrd pydrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7aa2d3b-bfd9-45e0-a267-610127b2d374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Declaracion de Librerías y Variables\n",
    "\n",
    "En estas dos celdas importamos las librerías de nuestro lenguaje (Python) y algunos frameworks como Pyspark y Pandas y definimos las variables:\n",
    "\n",
    "- Fecha de inicio del balance\n",
    "- Fecha fin del balance\n",
    "- Id del archivo publico de despacho de generacion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b09acc8f-202f-481e-982a-6de3f9d24e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql.functions import flatten,col,explode\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0de67f8c-a217-43da-9bdd-57d99b28acd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "\n",
    "En la siguiente celda completa los siguientes datos:\n",
    "\n",
    "1. Acorde a las fechas de tu balance y de tu archivo de generación, completa las fechas correspondientes **fecha_inicio** y **fecha_fin**\n",
    "2. Sube el archivo **DespachoGeneradoraX.xlsx** a una carpeta en Google Drive, en una carpeta pública, y obten el **id del archivo** (Averigua como con GPT, Copilot, o google)\n",
    "3. Abre una cuenta en **fastupload.io** y genera las llaves api para poder subir archivos, copia las api key en las variables abajo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdcb89ea-bcb7-41e0-aa0f-cfaed0f9aee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fecha_inicio=\"2024-06-09\"\n",
    "fecha_fin=\"2024-06-20\"\n",
    "id_archivo_balance_drive=\"\"\n",
    "api_key_1=\"\"\n",
    "api_key_2=\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab5b8334-2773-430d-a073-7f30f5fdec32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# (E) Fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f056f4-32f2-4ac2-ba92-d55e642a1410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (*E*) Extraccion del API de Precios de la Energía\n",
    "\n",
    "Aquí descargamos del API los datos en formato JSON, usamos una librería nativa de Python llamada Requests y con ella consumimos el api de la URL, y almacenamos el resultado en una estructura de datos denominada un dataframe, que servirá para procesamiento posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec2b8cbc-08c7-4ef7-a57a-1fa49d027f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = f\"https://www.simem.co/backend-files/api/PublicData?startDate={fecha_inicio}&endDate={fecha_fin}&datasetId=96D56E\"\n",
    "\n",
    "response = requests.get(url)\n",
    "dfPreciosBolsa=spark.read.json(sc.parallelize([response.text]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee2b738-4b25-4dfa-bf37-5b9cae3aa6a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "\n",
    "En la siguiente celda, realiza los pasos a continuación:\n",
    "\n",
    "1. Ve a la página del SIMEM.\n",
    "2. Navega y busca la página del catálogo de datos https://www.simem.co/pages/catalogodatos/51FC0A59-3A00-462C-B449-9CB8D5E007FB \n",
    "3. En la sección de datos, busca el dataset **\"Despacho programado recursos de generación\"**.\n",
    "4. Obten el **datasetId** y actualiza la siguiente celda de código.\n",
    "5. Completa el código de la celda, para que el resultado se guarde en el dataframe **dfDespachosUnidades**.\n",
    "6. Imprime el resultado del dataframe para ver que hayan datos.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c643dc-300f-4c0e-a899-2f5e5679f6e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (*E*) Extraccion del Plan de Despacho de Energía por Unidad y Central\n",
    "\n",
    "Aqui descargamos del API los datos del despacho en formato JSON, usamos una librería nativa de Python llamada Requests y con ella consumimos el api de la URL, y almacenamos el resultado en una estructura de datos denominada un dataframe, que servirá para procesamiento posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e715a9ee-423e-437b-bba9-76e011588c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datasetId=\"\"\n",
    "url = f\"https://www.simem.co/backend-files/api/PublicData?startdate={fecha_inicio}&enddate={fecha_fin}&datasetId={datasetId}}\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fffeadc-f8dd-4e33-93f8-784bf308e311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (*E*) Descarga del archivo con nuestros datos reales de generación\n",
    "\n",
    "En estas 2 celdas, descargamos de Google Drive nuestro archivo de consolidado de generación, igual que antes, usamos Requests y con ella descargamos el archivo de la URL, en este caso tenemos que traer el archivo a una ubicación local, y posteriormente lo cargamos con Pandas, un framework de procesamiento de datos de python para analítica, y lo cargamos como un dataframe de Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0d5ab44-324b-4a02-9831-39c67ea7fe7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "\n",
    "Realiza los pasos a continuación:\n",
    "\n",
    "1. Abre tu google drive.\n",
    "2. Sube el archivo de excel que estaba en el repositorio a una carpeta en tu drive. \n",
    "3. Temporalmente pon el archivo como **público** para poderlo descargar.\n",
    "4. Obten el ID del archivo y reemplazalo en la sección de variables **id_archivo_balance_drive**\n",
    "5. Ejecuta esa celda para actualizar las variables.\n",
    "6. Ejecuta la siguiente celda y verifica que se pueda leer el archivo con pandas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70eb0ba1-0392-4b77-a686-c4f9ebf1727d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "download_url = f'https://drive.google.com/uc?export=download&id={id_archivo_balance_drive}'\n",
    "local_file_path = '/tmp/balances3.xlsx'\n",
    "response = requests.get(download_url)\n",
    "with open(local_file_path, 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1437ea2-11a9-4d03-8ea9-f86eeef7a447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dfArchivoCapacidad = pd.read_excel(local_file_path,engine='openpyxl')\n",
    "display(dfArchivoCapacidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae08de9e-85d8-419c-91e4-d81efe0bcf28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# (T) Transformaciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd63cf2b-9c95-4ebb-ac52-62f2190dd724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filtrado de Columnas\n",
    "\n",
    "Nuestras fuentes, vienen en un estado crudo, o \"raw\", y hay datos, que no nos servirán para nuestro propósito, por lo cual, la primera transformación será quitar, o filtrar, los atributos que no se necesitan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585f62ec-500d-488f-b95a-d6f3b1f8455e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (*T*) Filtrado de los atributos que traen el precio de bolsa\n",
    "\n",
    "\n",
    "La documentación del API nos muestra que trae muchas columnas de información, denominados metadatos, necesitamos puntualmente los datos válidos para nuestro análisis. \n",
    "\n",
    "```\n",
    "CodigoDuracion:string\n",
    "CodigoPlanta:string\n",
    "FechaHora:string\n",
    "Valor:long\n",
    "```\n",
    "\n",
    "En estas 3 celdas, transformamos el formato utilizando funciones como aplanar o explotar, colocando alias y realizando un filtro o un select.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bacfea8d-7d85-482f-ac8e-4887c5e86e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfPreciosBolsaFiltrado=dfPreciosBolsa.select(explode(col(\"result.records\")).alias(\"results\")).select(col(\"results.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16be0ca6-4a2c-4656-a119-f5852c17e901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Esta celda sirve para mostrar los resultados del dataframe del Precio de Bolsa Filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04667a3b-6292-4c0a-9cc3-f4dba4ba2423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dfPreciosBolsaFiltrado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281188ad-e40f-47e0-acda-1ab5bd30988d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Esta celda registra el dataframe como una tabla o vista temporal **PrecioBolsa**, a la que podremos hacerle consultas SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8844411-1941-4198-8e1d-3cbd2fa45edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "En la siguiente celda, completa los siguientes pasos:\n",
    "\n",
    "1. Busca en internet como se crea una tabla temporal, a partir del dataframe **dfPreciosBolsaFiltrado** en Spark y crea una tabla denominada **PreciosBolsa**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3c9e10-a974-4df8-95e7-ae94e491fa23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Tu Código Aquí:\n",
    "##dfPreciosBolsaFiltrado.<Usa el Dataframe como base>\n",
    "##---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81309db-29c6-44ef-8e68-46ccc9634957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (*T*) Filtrado de los atributos que traen el despacho de las plantas\n",
    "\n",
    "Al igual que los precios de bolsa, el API de los despachos trae datos excesivos, que no necesitamos, vamos a extraer los datos de utilidad:\n",
    "\n",
    "\n",
    "```\n",
    "CodigoDuracion:string\n",
    "CodigoPlanta:string\n",
    "FechaHora:string\n",
    "Valor:long\n",
    "```\n",
    "Al igual que en el caso anterior, en las próximas 3 celdas aplanamos estructuras, renombramos con un alias y filtramos con una selección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a7b599-e945-4685-a9d6-3c8d573de746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfResultsDespachosUnidades=dfDespachosUnidades.select(explode(col(\"result.records\")).alias(\"results\")).select(col(\"results.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a250b450-3e19-410b-ac94-22925a2410a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Celda para mostrar los resultados del Despacho de las Unidades de Generación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5985809d-2c97-4ebd-88dc-006bdfba8e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "display(dfResultsDespachosUnidades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c6b89d-271e-48f4-b463-675bdccfcddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Celda donde registramos el dataframe con los datos descargados de los despachos en la tabla **Despachos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9238272-0e46-43ad-9c06-faff600eabf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "En la siguiente celda, completa los siguientes pasos:\n",
    "\n",
    "1. Busca en internet como se crea una tabla temporal a partir del dataframe **dfResultsDespachosUnidades** en Spark y crea una tabla denominada **despachos**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3664b9a2-438d-4938-b3a2-bedb4bbe3382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Tu código Aquí:\n",
    "#dfResultsDespachosUnidades.<Usa el dataframe como base>\n",
    "##----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b70fdce7-e330-46bf-ac0f-490a1499762a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (*T*) Filtrado de Plantas\n",
    "\n",
    "El despacho trae todas las plantas, de nuevo, tenemos que filtrar, pero ahora solo debemos traer las que nos interesan, por ejemplo Zipaquirá, Guavio, Quimbo y Chivor, de la Empresa de Energía ACME. Lo haremos con SQL.\n",
    "\n",
    "Luego guardaremos ese filtro en otro data frame, para delimitar el set de datos para calcular el balance con las plantas que nos importan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "521e57f6-c705-4115-84d3-f4079b7eb688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Primero diseñamos el SQL para filtrar con un condicional, las plantas que nos interesan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dec7a49-0e3b-4a68-9abb-bca7a40d3fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "\n",
    "Completa en las siguientes dos celdas, estos pasos:\n",
    "\n",
    "1. Completa la query en la siguiente celda, para que filtre de los despachos, los datos de las plantas que necesitas que están en el archivo de excel.\n",
    "2. Ejecuta la query y revisa los resultados, las plantas coinciden con las del archivo?.\n",
    "3. Cuando tu query funcione, en la celda siguiente, averigua como ejecutar en **spark** la query y guardar el resultado en un **dataframe** con el nombre **dfDespachosAcme**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbd6cf8-3207-4b38-8428-c822cc09b4be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Tu código SQL Aquí del punto 1:\n",
    "--SELECT * FROM Despachos WHERE <Condición del Filtro> order by Valor,FechaHora DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f0baca-7d22-49ce-a2d9-d3a4d09b873d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "La query resultante, la agregamos a **Spark**, para poderla trabajar con los otros datos en un dataframe llamado **dfDespachosAcme**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "924675c0-45eb-4245-8b22-20cc7df28be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Agrega el código del punto 3 aquí:\n",
    "#dfDespachosAcme="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b923445-2009-4d4c-a80b-98179f934156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### *Desafío*\n",
    "\n",
    "Logra generar **dfDespachosAcme** usando Pyspark o Pandas sin SQL!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e7036c-4fdf-4440-a116-8aec01a7d5fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Con esta celda desplegamos los resultados de los despachos filtrados por planta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a25f9877-e27d-4fc5-9468-3817c68e3f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "display(dfDespachosAcme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5482dad8-26bb-43c8-b992-dc2b87dc584d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Registramos los despachos filtrados por planta, en la tabla **dfCapacidadDespachosAcme**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bc3fcba-e229-461c-868f-e34a85909f78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "En la siguiente celda, completa los siguientes pasos:\n",
    "\n",
    "1. A partir del dataframe **dfDespachosAcme** en Spark y crea una tabla denominada **dfCapacidadDespachosAcme**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b12fedf7-eb43-4b6c-b25a-ddc4e304bb80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "#dfDespachosAcme.<Agrega tu código para crear la tabla aquí>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4892ccab-1546-401e-9fa4-4a15483debfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (*T*) Cambio de Formato Interno\n",
    "\n",
    "Ahora, **cambiaremos** el archivo de excel con nuestra capacidad, que fue cargado como una estructura de **Pandas, a una estructura de Spark**, para poder usar SQL con los datos descargados desde XM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daeb6ecc-1d6d-4d63-8718-72cc5f2fb027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para transformarlos, **aplicaremos un esquema, que aplicará unos tipos de dato, y les dará nombres a las columnas o atributos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae5fad1e-265c-44c2-8ec3-8e45c3be5483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "\n",
    "1. A partir del dataframe **dfTransformadoCapacidad** en Spark y crea una tabla denominada **CapacidadArchivo**, en la siguiente celda.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d052bc-53aa-4b28-b881-d427962bddde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schemaArchivo = StructType([\n",
    "    StructField(\"useless\", StringType(), True),\n",
    "    StructField(\"fecha\", StringType(), True),\n",
    "    StructField(\"planta\", StringType(), True),\n",
    "    StructField(\"generador\", StringType(), True),\n",
    "    StructField(\"capacidad\", StringType(), True),\n",
    "    StructField(\"codigo\", StringType(), True)\n",
    "])\n",
    "dfTransformadoCapacidad = spark.createDataFrame(dfArchivoCapacidad,schema=schemaArchivo)\n",
    "#dfTransformadoCapacidad.<Agrega tu código Aquí>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4646358-e3c8-4af6-9a60-49fee543f1a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (*T*) Vamos a extraer las fechas del archivo de capacidades y quitar con un filtro las celdas que no nos sirven\n",
    "\n",
    "El archivo de Excel **trae muchos datos innecesarios**, debemos **quitar esas celdas**, y extraer las fechas para poder utilizar esos datos del archivo con los datos del ente regulador (xm), eso lo haremos con SQL con algunas técnicas:\n",
    "\n",
    "- Expresiones regulares.\n",
    "- Sustituciones de texto.\n",
    "- Condicionales.\n",
    "\n",
    "Y posteriormente lo volvemos a dejar en un dataframe listo para utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22b3bf8-a585-4ffc-88ea-cc0801f525e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Primero, filtramos con SQL, las celdas que no son vacías o nulas, donde el generador no esté vacío, y omitimos otros caracteres, de paso extraemos através de expresiones regulares, el día, el mes, el año y la hora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23ae2dc2-313e-4ba0-b24f-792adc8301ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "\n",
    "1. Lee la siguiente Query, crees que se podría realizar de otra forma?, analízala, crea una copia de la celda, modifícala, compara los resultados.\n",
    "2. Usando **Spark** basado en el dataframe **dfArchivoCapacidadLimpiado** crea la tabla temporal **ArchivoCapacidadLimpiado**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c62b5e3-8395-4c1a-9142-522d5bf09e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT  b.anio,b.mes,b.dia,b.hora, b.codigo, b.capacidad  FROM \n",
    "(SELECT SUBSTR(A.FECHA, instr(A.fecha,\"YEAR\")+5,4) AS anio, \n",
    "regexp_replace(SUBSTR(A.FECHA, instr(A.fecha,\"MONTH\")+6,2),',','') AS mes,\n",
    "regexp_replace(SUBSTR(A.FECHA, instr(A.fecha,\"DAY_OF_MONTH\")+13,2),',','') AS dia,\n",
    "regexp_replace(SUBSTR(A.FECHA, instr(A.fecha,\"HOUR_OF_DAY\")+12,2),',','') AS hora,\n",
    "* FROM CapacidadArchivo a WHERE generador IS NOT NULL AND generador != \"NaN\" AND generador != \"GENERADOR\") b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ed246af-1ead-40ef-bde5-976177279684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Una vez validada nuestra query, usamos Spark para crear el dataframe **dfArchivoCapacidadLimpiado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f9927b2-cd36-4e15-afcc-16e0375f8f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfArchivoCapacidadLimpiado=spark.sql(\"\"\"SELECT  b.anio,b.mes,b.dia,b.hora, b.codigo, b.capacidad  FROM \n",
    "(SELECT SUBSTR(A.FECHA, instr(A.fecha,\"YEAR\")+5,4) AS anio, \n",
    "regexp_replace(SUBSTR(A.FECHA, instr(A.fecha,\"MONTH\")+6,2),',','') AS mes,\n",
    "regexp_replace(SUBSTR(A.FECHA, instr(A.fecha,\"DAY_OF_MONTH\")+13,2),',','') AS dia,\n",
    "regexp_replace(SUBSTR(A.FECHA, instr(A.fecha,\"HOUR_OF_DAY\")+12,2),',','') AS hora,\n",
    "* FROM CapacidadArchivo a WHERE generador IS NOT NULL AND generador != \"NaN\" AND generador != \"GENERADOR\") \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22b5da2-df84-4f1e-9058-9c84f73198fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Con el Dataframe, Creamos la tabla temporal **ArchivoCapacidadLimpiado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f9b261-4616-4f05-9814-cc63f06789d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Tu código Aquí:\n",
    "#dfArchivoCapacidadLimpiado.<Tu código para crear la tabla temporal aquí>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603056ea-4dfd-4793-99a0-aa22ab2c99a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paso siguiente, de la tabla con los despachos programados del ente regulador **dfCapacidadDespachosAcme**, vamos a extraer las columnas o atributos con los que vamos a cruzar la información: día, mes, año, hora y valor, junto al código de la planta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18a737a3-de26-4140-a403-c2e0a3e5875b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT  day(FechaHora) as dia_despacho, month(FechaHora) as mes_despacho, year(FechaHora) as anio_despacho,hour(FechaHora) as hora_despacho, Valor as capacidad, CodigoPlanta as codigo FROM dfCapacidadDespachosAcme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d76a9ab-4c31-4421-8027-73602c492125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Guardamos el SQL como un dataframe **dfDespachosAcmeTransformado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25cfbf3c-3d4b-4979-a60d-be67a732b478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfDespachosAcmeTransformado=spark.sql(\"\"\"SELECT  day(FechaHora) as dia_despacho, month(FechaHora) as mes_despacho, year(FechaHora) as anio_despacho,hour(FechaHora) as hora_despacho, Valor as capacidad, CodigoPlanta as codigo FROM dfCapacidadDespachosAcme\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe2e516-fb7a-4792-9637-9aef7806daa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Y lo registramos como una tabla **DespachosAcmeTransformado**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfe53b21-e36f-4f09-9a4d-c53f5ea4c573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "1. Agrega como es usual, el código para crear la tabla **DespachosAcmeTransformado** a partir del dataframe **dfDespachosAcmeTransformado**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2a1f27-ac3d-4d69-8b0b-0535ea2e6ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Tu código Aquí para crear la tabla temporal\n",
    "#dfDespachosAcmeTransformado.<Completa aquí>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30cacfa4-bc7d-49ad-b31f-56ca0e303def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (*T*) Cálculo del balance de energía.\n",
    "\n",
    "Ya tenemos dos dataframes, uno con los datos del despacho que vienen del ente regulador **DespachosAcmeTransformado** y otro con los datos limpios de nuestro archivo de excel **ArchivoCapacidadLimpiado**, en esta sección, usaremos SQL para restar el despacho programado, de nuestra capacidad por hora, y luego realizamos una sumatoria para sacar el **balance consolidado de energía por planta y unidad**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9579c48-f588-4192-8c95-4d46a0a7de49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Como es usual, primero diseñamos la query en SQL, dentro del parentesis grande la **subquery** une la tabla del despacho del regulador, con el archivo de capacidades, usando el **código, el año, el mes, el día y la hora**.\n",
    "\n",
    "Posteriormente, convertimos la **capacidad** que viene como texto con **cast** y restamos el valor de la **capacidad reportada por el regulador**, de la **capacidad real de mi archivo**, y eso lo llamamos **balance_disponible_horario**.\n",
    "\n",
    "En la query externa (fuera del parentesis), sacamos la **Sumatoria con SUM** y ese es el valor **consolidado del día de nuestra planta en Kilowatts Hora**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c3be7bc-5ee5-4999-99ce-8e0725e0d112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "\n",
    "1. En la siguiente celda toma la query y completa luego del **ON** las condiciones para unir **a** con **b**, ej: **a.atributo >= b.atributo**, recuerda encadenarlos con un **AND**\n",
    "2. **Prueba tu query** y valida que te esta trayendo los datos de balances para las plantas y las columnas **anio**, **mes**, **día**, **código** y **consolidado_planta**.\n",
    "3. En la celda que le sigue completa el código para incorporar la **query que acabas de probar** y generar el dataframe **dfBalanceConsolidado**.\n",
    "4. Crea la tabla temporal **BalanceConsolidado** del dataframe **dfBalanceConsolidado**.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e332f71f-c5ff-4e32-9363-9e692477a2e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT c.anio, c.mes, c.dia, c.codigo, SUM(c.balance_disponible_horario) AS consolidado_planta\n",
    "FROM\n",
    "(\n",
    "  SELECT a.anio, a.mes, a .dia, a.hora, a.codigo, cast(a.capacidad AS DECIMAL) - cast(b.capacidad as DECIMAL) as balance_disponible_horario FROM ArchivoCapacidadLimpiado a JOIN DespachosAcmeTransformado b\n",
    "  ON\n",
    "  --Tu código aquí--\n",
    ") c \n",
    "GROUP BY c.anio, c.mes, c.dia, c.codigo \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d475f187-818f-41a5-90c9-8337d6145b74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## *Desafío*\n",
    "\n",
    "Realiza la misma query usando solo **Pyspark**, puedes hacerlo en una sola etapa o en 2 separadas, GO!.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fea11af-4418-4f5b-b78c-b38c9e53b891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Si la query anterior funciona, la guardamos con Spark, en el dataframe **dfBalanceConsolidado** y creamos la tabla **BalanceConsolidado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5960c4-67c8-4984-a9f7-8fcb5b08ab24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Tu código va aquí\n",
    "#dfBalanceConsolidado = <Aqui usa spark para crear el dataframe>\n",
    "#dfBalanceConsolidado.<Aqui a partir del dataframe crea la tabla.>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a83de3-a9d2-46f3-a083-ebc76a797a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ahora con el precio de la bolsa que está en la tabla **Precios Bolsa** vamos a **multiplicar** de la capacidad en el **Balance Consolidado** que acabamos de calcular y eso nos dará el valor de la energía en miles de millones de pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1389744-124a-4298-85b2-83c7b27f3527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Ejercicio\n",
    "\n",
    "En la siguiente query, a veces salen valores nulos, porque?.\n",
    "Que cambios usarías para evitar que a veces salgan nulos?.\n",
    "Implementa los cambios!.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8669227b-df7a-43e6-9be5-22a3bcde50e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT c.anio, c.mes, c.dia, c.codigo, c.consolidado_planta, (c.consolidado_planta * b.valor)/1000 as Compromisos_MCOP\n",
    "FROM BalanceConsolidado c JOIN \n",
    "(\n",
    "SELECT DAY(a.Fecha) as dia,Month(a.Fecha) as mes,Year(a.Fecha) as anio, a.Valor as valor \n",
    "FROM PreciosBolsa a \n",
    "WHERE a.CodigoVariable =\"PPBOGReal\" AND Version=\"TXR\"\n",
    ") b \n",
    "ON c.dia = b.dia AND\n",
    "c.mes = b.mes AND\n",
    "c.anio = b.anio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0492a18-736a-420e-8d55-b49ad6330749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aqui guardamos lo que hicimos en SQL en el dataframe **BalancesComprasVentasEnergía**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b75b0612-b4c0-45eb-aa4a-bd2232c33aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "1. Usa la query anterior y completa la creación del dataframe **dfBalancesComprasVentasEnergia**\n",
    "2. Usa el dataframe como es usual y crea la tabla temporal **BalancesComprasVentasEnergia**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af15fcfa-dcab-469e-aff9-832bff938439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Tu código Aquí\n",
    "#dfBalancesComprasVentasEnergia=<Aca el código para crear el dataframe de spark>\n",
    "#dfBalancesComprasVentasEnergia.<Crea la tabla BalancesComprasVentasEnergia>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1814e635-dc94-4aa6-8f43-80e1ce9c4327",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finalmente, para crear un reporte que vamos a enviar al regulador, vamos a agregar una columna **Operación**, que consistirá en evaluar, si el **compromiso es negativo**, debemos comprar energía (la planta no genera lo planeado y hay que ir a bolsa), si el **compromiso es positivo**, tenemos un excedente, podemos vender esa energía en bolsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3204428b-f5b8-4202-97af-fa2d5828aea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT a.*, CASE WHEN a.Compromisos_MCOP < 0 THEN \"Comprar\" ELSE \"Vender\" END as Operacion FROM BalancesComprasVentasEnergia a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b29945-7010-4dc4-b5af-52b79df66e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Preparamos el Data Frame de Salida **dfReporteCompraVentaEnergiaAcme** para poderlo enviar en nuestra etapa **L** al destino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b99e6a43-594c-48aa-badf-33b33c3915ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Ejercicio\n",
    "1. Toma la query anterior y crea el dataframe que vamos a convertir en el reporte del balance **dfReporteCompraVentaEnergiaAcme**\n",
    "2. Ejecuta la celda con display, y mira los resultados, tienen sentido?, que ha pasado?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf7df0f-39e8-4ea4-8874-a6ecc7129e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dfReporteCompraVentaEnergiaAcme=<Usa la query anterior para crear el dataframe aqui>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb82c9c-1a5e-475e-a79f-5b34d44fb7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dfReporteCompraVentaEnergiaAcme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22341aa-e557-4b0e-afc7-9092f1d421a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##(*L*) Transferencia de Archivo de Resultados a Destino\n",
    "\n",
    "Ahora tomamos el resultado de las compras y ventas de energía y lo transferimos al destino para que lo cargue el usuario final.\n",
    "\n",
    "Vamos a simular la carga del archivo a un servicio de transferencia de archivos llamado https://fastupload.io "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4144bdad-e162-4a06-b0a4-bb10c24a3633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vamos a **obtener el CSV**, para ello vamos a usar python para **validar si nuestro archivo es un CSV y si es, lo copiamos** a una ruta que podamos acceder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6fe006-5f74-4a4d-b633-bb0bbe13f162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "local_csv_path = \"dbfs:/reporteSalidaRegulador/\"\n",
    "dfReporteCompraVentaEnergiaAcme.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(local_csv_path)\n",
    "filepath=\"\"\n",
    "for archivo in dbutils.fs.ls(\"/reporteSalidaRegulador\"):\n",
    " if '.csv' in archivo.name:\n",
    "    filepath=archivo.path\n",
    "print(filepath)\n",
    "\n",
    "dbutils.fs.cp(filepath, f\"file:/tmp/dfReporteCompraVentaEnergiaAcme.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351db18d-74f9-4d54-81cb-014328401635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## *(L)* Autenticación en el sitio remoto del ente regulador\n",
    "\n",
    "Para poder **cargar archivos**, hay que **iniciar sesión** en la plataforma, para ello se utiliza un API, https://fastupload.io/api#authorize , en el api, la plataforma iniciará sesión y obtendrá un token (secuencia de texto y números) que nos permitirá subir el archivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58b96dae-275e-439d-93de-5a0606066626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "##Ejercicio\n",
    "\n",
    "1. En el siguiente código, utiliza el objeto **json_response** y carga en el, el texto del **response** obtenido, en formato **json** (puedes buscar en internet o consultar).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fbc2fbc-3636-446c-9673-07cb0265dbd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_upload_access_token = \"\"\n",
    "file_upload_account_id = \"\"\n",
    "params = {'key1': api_key_1, 'key2': api_key_2}\n",
    "response=requests.get(\"https://fastupload.io/api/v2/authorize\", params)\n",
    "#json_response = <Acá va tu código que toma el response, lo convierte en JSON y lo sube a json_response>\n",
    "try:\n",
    "  file_upload_access_token = json_response[\"data\"][\"access_token\"]\n",
    "  file_upload_account_id = json_response[\"data\"][\"account_id\"]\n",
    "except:\n",
    "  print(\"Error autenticando y autorizando en el servicio remoto de carga de archivos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8283276c-e7d5-492c-9af2-e6386b976bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Así se ve un token de un API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6510dba-4ae8-4aa1-a56c-b70b4839127c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(file_upload_access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a63e950-037e-47dd-a8a9-7db9564a5424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## *(L)* Ahora vamos a cargar el Archivo finalmente con el token\n",
    "\n",
    "Para ello usamos otra API https://fastupload.io/api#file-upload, la cual nos pide algunos parámetros, como el nombre del archivo, el token, la cuenta y el folder, al final, al imprimir la respuesta, veremos que el arhivo fue cargado \"File Uploaded\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92230ccc-c470-4e5b-828f-0f7350e59539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "upload_folder_id = \"\"\n",
    "json_response=\"\"\n",
    "with open(\"/tmp/dfReporteCompraVentaEnergiaAcme.csv\", \"rb\") as archivo:\n",
    "  files = {\"upload_file\": (archivo.name, archivo)}  # Create a dictionary for the file upload\n",
    "\n",
    "  data = {\n",
    "    \"access_token\": file_upload_access_token,\n",
    "    \"account_id\": file_upload_account_id,\n",
    "    \"folder_id\": upload_folder_id  # Include folder_id if provided\n",
    "  }\n",
    "\n",
    "  response = requests.post(\"https://fastupload.io/api/v2/file/upload\", files=files, data=data)\n",
    "  \n",
    "print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1068438123068544,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "EONTEC - Demo POC ETL Gestión de la Energía - Estudiantes",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
